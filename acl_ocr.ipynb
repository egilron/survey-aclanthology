{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR\n",
    "with Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Initialize the client\n",
    "api_key = (Path.home() / \".keys\" / \"mistral\").read_text().strip()\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def ocr_pdf_from_url(pdf_url: str):\n",
    "    \"\"\"\n",
    "    Performs OCR on a remote PDF using Mistral's API.\n",
    "    \"\"\"\n",
    "    response = client.ocr.process(\n",
    "        model=\"mistral-ocr-latest\",\n",
    "        document={\n",
    "            \"type\": \"document_url\",\n",
    "            \"document_url\": pdf_url\n",
    "        },\n",
    "        include_image_base64=False  # Set to True if you want to extract images\n",
    "    )\n",
    "    \n",
    "    # The response contains 'pages' with structured markdown for each page\n",
    "    return response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "if False:\n",
    "    url = \"https://aclanthology.org/2022.coling-1.589.pdf\"  # Attention Is All You Need paper\n",
    "    ocr_result = ocr_pdf_from_url(url)\n",
    "    all_pages = [p.markdown for p in ocr_result.pages]\n",
    "    pages_md = \"\\n\".join(all_pages).split(\"# References\")[0].strip()\n",
    "    Path(\"attention.md\").write_text(pages_md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the entire acl list of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs, was 1130\n",
      "Docs, remaining 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_and_save(docs, output_path):\n",
    "    \"\"\"The docs list will be checked here \"\"\"  \n",
    "    if Path(output_path).exists():  \n",
    "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            completed = [json.loads(line) for line in f]\n",
    "    else:\n",
    "        completed = [{}]\n",
    "\n",
    "    completed_urls = [c.get(\"url\") for c in completed if c.get(\"status\") == \"success\"]\n",
    "    print(\"Docs, was\", len(docs))\n",
    "    docs = [d for d in docs if d[\"url\"] not in completed_urls]\n",
    "    print(\"Docs, remaining\", len(docs))\n",
    "\n",
    "    for doc in tqdm.tqdm(docs):     \n",
    "        try:\n",
    "            # print(\"Now processing\",doc.get(\"bibkey\"))\n",
    "            ocr_response = ocr_pdf_from_url(doc[\"url\"])\n",
    "            \n",
    "            pages_md = [page.markdown for page in ocr_response.pages]\n",
    "            fulltext = \"\\n\\n\".join([p for p in pages_md])\n",
    "            cleantext = fulltext.split(\"# References\")[0].strip()\n",
    "            ocr_data = dict(\n",
    "                fulltext = fulltext,\n",
    "                cleantext = cleantext,\n",
    "                pages_md = pages_md\n",
    "            )\n",
    "            doc = {**doc, **ocr_data}\n",
    "            \n",
    "            doc[\"status\"] = \"success\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e)[:50])\n",
    "            doc[\"status\"] = \"failed\"\n",
    "        \n",
    "        # OPEN, WRITE, CLOSE for every single document\n",
    "        with open(output_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    " \n",
    "\n",
    "source = pd.read_parquet(\"sapapers.parquet\").to_dict('records')\n",
    "\n",
    "process_and_save(source, \"acl_sapapers.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " semeval >>\n",
      " False\n",
      "\n",
      "\n",
      " id >>\n",
      " 897\n",
      "\n",
      "\n",
      " bibkey >>\n",
      " cheng-etal-2024-learning\n",
      "\n",
      "\n",
      " year >>\n",
      " 2024\n",
      "\n",
      "\n",
      " url >>\n",
      " https://aclanthology.org/2024.lrec-main.897.pdf\n",
      "\n",
      "\n",
      " fulltext >>\n",
      " # Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis\n",
      "\n",
      "Zhenxiao Cheng $^{1}$ , Jie Zhou $^{1,*}$ , Wen Wu $^{1}$ , Qin Chen $^{1}$ , Liang He $^{1}$\n",
      "\n",
      "$^{1}$  School of Computer Science and Technology, East China Normal University, Shanghai, China\n",
      "\n",
      "\n",
      "--- istics, pages 2152–2161.\n",
      "\n",
      "Jie Zhou, Qi Zhang, Qin Chen, Liang He, and Xuan-Jing Huang. 2022. A multi-format transfer learning model for event argument extraction via variational information bottleneck. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1990–2000.\n",
      "\n",
      "\n",
      " cleantext >>\n",
      " # Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis\n",
      "\n",
      "Zhenxiao Cheng $^{1}$ , Jie Zhou $^{1,*}$ , Wen Wu $^{1}$ , Qin Chen $^{1}$ , Liang He $^{1}$\n",
      "\n",
      "$^{1}$  School of Computer Science and Technology, East China Normal University, Shanghai, China\n",
      "\n",
      "\n",
      "--- the National Natural Science Foundation of China (No.62307028 and No.62377013), the Science and Technology Commission of Shanghai Municipality Grant (No.22511105901, No.21511100402 and No.21511100302), and Shanghai Science and Technology Innovation Action Plan (No.23ZR1441800 and No.23YF1426100).\n",
      "\n",
      "#\n",
      "\n",
      "\n",
      " pages_md >>\n",
      " ['# Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis\\n\\nZhenxiao Cheng $^{1}$ , Jie Zhou $^{1,*}$ , Wen Wu $^{1}$ , Qin Chen $^{1}$ , Liang He $^{1}$\\n\\n$^{1}$  School of Computer Science and Technology, East China Normal University, Shanghai, C\n",
      "--- cs, pages 2152–2161.\\n\\nJie Zhou, Qi Zhang, Qin Chen, Liang He, and Xuan-Jing Huang. 2022. A multi-format transfer learning model for event argument extraction via variational information bottleneck. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1990–2000.']\n",
      "\n",
      "\n",
      " status >>\n",
      " success\n"
     ]
    }
   ],
   "source": [
    "# Inspect\n",
    "with open(\"acl_sapapers.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    completed = [json.loads(line) for line in f]\n",
    "for key, value in completed[-3].items():\n",
    "    value = str(value)\n",
    "    print(\"\\n\\n\",key,\">>\\n\", value[:300])\n",
    "    if len(value) > 200: print (\"---\",value[-300:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aclanthology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
